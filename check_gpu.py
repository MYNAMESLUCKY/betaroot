# ğŸ® GPU Detection and Status Check
import tensorflow as tf
import subprocess
import sys
import os

print('ğŸ® GPU DETECTION AND STATUS')
print('=' * 40)

def check_gpu_status():
    """Check GPU status and configuration"""
    
    print('\nğŸ” TensorFlow GPU Detection:')
    print('-' * 30)
    
    # Check TensorFlow GPU availability
    gpus = tf.config.list_physical_devices('GPU')
    
    if gpus:
        print(f'âœ… GPU(s) Detected: {len(gpus)}')
        for i, gpu in enumerate(gpus):
            print(f'   GPU {i}: {gpu.name}')
            print(f'   Type: {gpu.device_type}')
            
            # Get GPU details
            try:
                details = tf.config.experimental.get_device_details(gpu)
                print(f'   Name: {details.get("device_name", "Unknown")}')
                print(f'   Compute Capability: {details.get("compute_capability", "Unknown")}')
            except:
                print('   Details: Not available')
    else:
        print('âŒ No GPU detected by TensorFlow')
        print('   Using CPU for training (slower)')
    
    print(f'\nğŸ® GPU Configuration:')
    print('-' * 30)
    
    if gpus:
        # Configure GPU memory growth
        for gpu in gpus:
            try:
                tf.config.experimental.set_memory_growth(gpu, True)
                print(f'âœ… Memory growth enabled for {gpu.name}')
            except:
                print(f'âš ï¸  Could not enable memory growth for {gpu.name}')
    
    # Check CUDA availability
    print(f'\nğŸ”¥ CUDA Status:')
    print('-' * 30)
    try:
        # Check if CUDA is available
        from tensorflow.python.platform import build_info as build
        cuda_version = build.cuda_version
        cudnn_version = build.cudnn_version
        print(f'âœ… CUDA Version: {cuda_version}')
        print(f'âœ… cuDNN Version: {cudnn_version}')
    except:
        print('âš ï¸  CUDA information not available')
    
    # Check NVIDIA drivers (Windows)
    print(f'\nğŸ–¥ï¸  NVIDIA Driver Status:')
    print('-' * 30)
    try:
        # Try to get NVIDIA driver info
        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True, timeout=5)
        if result.returncode == 0:
            print('âœ… NVIDIA drivers installed')
            
            # Parse nvidia-smi output
            lines = result.stdout.split('\n')
            for line in lines:
                if 'RTX 2050' in line or 'GeForce' in line:
                    print(f'ğŸ¯ GPU Found: {line.strip()}')
                elif 'CUDA Version' in line:
                    print(f'ğŸ”¥ {line.strip()}')
                elif 'Driver Version' in line:
                    print(f'ğŸ“± {line.strip()}')
        else:
            print('âŒ NVIDIA drivers not found or nvidia-smi not available')
    except:
        print('âŒ Could not check NVIDIA drivers')
        print('ğŸ’¡ Install NVIDIA drivers for GPU acceleration')
    
    # Check current platform
    print(f'\nğŸŒ Platform Detection:')
    print('-' * 30)
    
    # Check for Google Colab
    try:
        import google.colab
        print('âœ… Running on Google Colab')
    except ImportError:
        print('âŒ Not running on Google Colab')
    
    # Check for Kaggle
    if 'KAGGLE_KERNEL_RUN_TYPE' in os.environ:
        print('âœ… Running on Kaggle')
    else:
        print('âŒ Not running on Kaggle')
    
    # Check for local environment
    if not 'KAGGLE_KERNEL_RUN_TYPE' in os.environ:
        try:
            import google.colab
        except ImportError:
            print('âœ… Running on local machine')
    
    # Memory info
    print(f'\nğŸ’¾ Memory Status:')
    print('-' * 30)
    
    if gpus:
        try:
            # Get GPU memory info
            for i, gpu in enumerate(gpus):
                memory_info = tf.config.experimental.get_memory_info(gpu)
                print(f'GPU {i} Memory:')
                print(f'   Current: {memory_info.get("current", 0) / 1024**2:.1f} MB')
                print(f'   Peak: {memory_info.get("peak", 0) / 1024**2:.1f} MB')
        except:
            print('âš ï¸  GPU memory info not available')
    
    # Test GPU with simple operation
    print(f'\nâš¡ GPU Performance Test:')
    print('-' * 30)
    
    if gpus:
        try:
            import time
            import numpy as np
            
            # Test matrix multiplication on GPU
            print('Testing GPU performance...')
            
            with tf.device('/GPU:0' if gpus else '/CPU:0'):
                # Create large matrices
                size = 1000
                a = tf.random.normal((size, size))
                b = tf.random.normal((size, size))
                
                # Time the operation
                start_time = time.time()
                c = tf.matmul(a, b)
                result = c.numpy()
                gpu_time = time.time() - start_time
                
                print(f'âœ… GPU Matrix Multiplication: {gpu_time:.3f} seconds')
                print(f'   Matrix size: {size}x{size}')
                print(f'   Device: {"GPU" if gpus else "CPU"}')
            
            # Compare with CPU if GPU is available
            if gpus:
                with tf.device('/CPU:0'):
                    start_time = time.time()
                    c_cpu = tf.matmul(a, b)
                    result_cpu = c_cpu.numpy()
                    cpu_time = time.time() - start_time
                
                speedup = cpu_time / gpu_time
                print(f'ğŸ“Š GPU Speedup: {speedup:.2f}x faster than CPU')
                
        except Exception as e:
            print(f'âŒ Performance test failed: {e}')
    else:
        print('âš ï¸  No GPU available for performance test')
    
    return len(gpus) > 0

def check_training_gpu_usage():
    """Check if training will use GPU"""
    print(f'\nğŸš€ Training GPU Usage:')
    print('-' * 30)
    
    gpus = tf.config.list_physical_devices('GPU')
    
    if gpus:
        print('âœ… Models will train on GPU')
        print(f'ğŸ¯ Available GPUs: {len(gpus)}')
        
        # Show GPU memory configuration
        for i, gpu in enumerate(gpus):
            try:
                # Check current memory growth setting
                print(f'GPU {i}: Memory growth enabled')
            except:
                print(f'GPU {i}: Memory growth not configured')
        
        print('\nğŸ’¡ Tips for GPU training:')
        print('   - Use larger batch sizes for better GPU utilization')
        print('   - Monitor GPU memory usage')
        print('   - Enable mixed precision for faster training')
        
    else:
        print('âŒ Models will train on CPU')
        print('âš ï¸  Training will be slower without GPU acceleration')
        print('\nğŸ’¡ To enable GPU training:')
        print('   1. Install NVIDIA drivers')
        print('   2. Install CUDA toolkit')
        print('   3. Install cuDNN')
        print('   4. Ensure TensorFlow GPU version is installed')

def main():
    """Main function"""
    print('ğŸ® DEFENSE INTELLIGENCE - GPU STATUS CHECK')
    print('=' * 50)
    
    # Check GPU status
    gpu_available = check_gpu_status()
    
    # Check training GPU usage
    check_training_gpu_usage()
    
    # Summary
    print(f'\nğŸ‰ GPU STATUS SUMMARY:')
    print('=' * 30)
    
    if gpu_available:
        print('âœ… GPU Available: YES')
        print('ğŸš€ Training Acceleration: ENABLED')
        print('âš¡ Performance: OPTIMIZED')
        print('ğŸ¯ Your RTX 2050 should be utilized!')
    else:
        print('âŒ GPU Available: NO')
        print('ğŸŒ Training Acceleration: DISABLED')
        print('âš¡ Performance: CPU ONLY')
        print('ğŸ’¡ Install NVIDIA drivers to enable GPU')
    
    print(f'\nğŸ”¥ TensorFlow Version: {tf.__version__}')
    print(f'ğŸ“ Platform: {"Local" if not "KAGGLE_KERNEL_RUN_TYPE" in os.environ else "Cloud"}')

if __name__ == '__main__':
    main()
